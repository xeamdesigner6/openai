import React, { useState, useRef, useEffect, useCallback } from 'react';
import {
  FaMicrophone,
  FaMicrophoneSlash,
  FaVideo,
  FaVideoSlash,
} from 'react-icons/fa'; // Importing icons
import { RealtimeClient } from '@openai/realtime-api-beta';
import { ItemType } from '@openai/realtime-api-beta/dist/lib/client.js';
import TimerComponent from '../components/timer/TimerComponent';
import imagess from '../assests/imgds.jpg';
import data from '../data/data';
import WavEncoder from 'wav-encoder'; // Import wav-encoder
import { WavRecorder, WavStreamPlayer } from '../lib/wavtools/index.js';
import { Link } from 'react-router-dom';
import DeleteModel from '../components/delete/DeleteModel';
import { toast } from 'react-toastify';
import { log } from 'console';

const LOCAL_RELAY_SERVER_URL: string =
  process.env.REACT_APP_LOCAL_RELAY_SERVER_URL || '';

declare global {
  interface Window {
    SpeechRecognition: any;
    webkitSpeechRecognition: any;
  }
}

interface SpeechRecognitionEvent extends Event {
  results: SpeechRecognitionResultList;
}

/**
 * Type for all event logs
 */
interface RealtimeEvent {
  time: string;
  source: 'client' | 'server';
  count?: number;
  event: { [key: string]: any };
}

function ScenarioForm() {
  const [isRecording, setIsRecording] = useState(false);
  const [mediaStream, setMediaStream] = useState<MediaStream | null>(null);
  const [videoBlob, setVideoBlob] = useState<Blob | null>(null);

  const recordedChunksRef = useRef<Blob[]>([]); 
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  
  const [isCameraOn, setIsCameraOn] = useState(true); // State for camera
  const [isMicOn, setIsMicOn] = useState(true); // State for microphone

  const [items, setItems] = useState<ItemType[]>([]);
  const [chats, setChats] = useState<any[]>([]); // Assuming chats is an array
  const [botChat, setBotChat] = useState<any>(null); // Assuming chats is an array
  const [isOpen, setIsOpen] = useState(false);
  const [getTips, setgetTips] = useState<any | null>(null);
  const [text, setText] = useState<string>('');
  const [videoUrl, setVideoUrl] = useState<string | undefined>("");

  const videoChunksRef = useRef<Blob[]>([]);
  
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const streamRef = useRef<MediaStream | null>(null);

  const [isConnected, setIsConnected] = useState(false);
  const startTimeRef = useRef<string>(new Date().toISOString());
  const [timeData, setTimeData] = useState<{
    hours: number;
    minutes: number;
    seconds: number;
  }>({
    hours: 0,
    minutes: 0,
    seconds: 0,
  });
  const [time, setTime] = useState<string>('');
  const [timeUpdated, setTimeUpdated] = useState<boolean>(false);

  const [includeVideo, setIncludeVideo] = useState(false);
  const mediaStreamRef = useRef(null);

  const SpeechRecognition =
    (window as any).SpeechRecognition ||
    (window as any).webkitSpeechRecognition;

  const recognition = new SpeechRecognition();
  recognition.lang = 'en-US'; // Set the language
  recognition.interimResults = false; // Only return final results
  recognition.continuous = false; // Listen for a single phrase

  const wavRecorderRef = useRef<WavRecorder>(
    new WavRecorder({ sampleRate: 24000 })
  );
  const wavStreamPlayerRef = useRef<WavStreamPlayer>(
    new WavStreamPlayer({ sampleRate: 24000 })
  );

  const clientRef = useRef<RealtimeClient>(
    new RealtimeClient(
      LOCAL_RELAY_SERVER_URL
        ? { url: LOCAL_RELAY_SERVER_URL }
        : {
            apiKey: process.env.OPENAI_API_KEY,
            dangerouslyAllowAPIKeyInBrowser: true,
          }
    )
  );

  useEffect(() => {
    return () => {
      stopMediaStream();
    };
  }, []);

  const startMediaStream = async () => {
    try {
      const client = clientRef.current;
      const constraints: MediaStreamConstraints = {
        audio: true,
        video: true,
      };
      const stream = await navigator.mediaDevices.getUserMedia(constraints);
      if (isCameraOn && videoRef.current) {
        videoRef.current.srcObject = stream;
        videoRef.current.play(); // Ensure the video plays
      }
      
      // Setup Web Audio API for voice detection
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;

      source.connect(analyser);
      analyserRef.current = analyser;
      audioContextRef.current = audioContext;

      
      const mediaRecorder = new MediaRecorder(stream);

      mediaRecorderRef.current = mediaRecorder;
      
      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          console.log("@@@ DATA SIZE: : ", e.data);
          videoChunksRef.current.push(e.data);
        }
      };
      
      mediaRecorder.onstop = async () => {

      }

      mediaRecorderRef.current.start();

      monitorAudioLevels();

      // Client connection logic (Placeholder)
      if (client && client.isConnected()) {
        console.log('Client already connected. Skipping connect()..');
      } else {
        const response = await client.connect();
        console.log('Client not connected. Establishing connection... ', response.valueOf());
      }
    } catch (error) {
      console.error("Error accessing media devices:", error);
    }
  };

  const stopMediaStream = async() => {
    mediaStream?.getTracks().forEach((track) => track.stop());
    setMediaStream(null);
      analyserRef.current?.disconnect();
      analyserRef.current = null;
      audioContextRef.current?.close();
      audioContextRef.current = null;
    if (silenceTimeoutRef.current) {
      clearTimeout(silenceTimeoutRef.current);
      silenceTimeoutRef.current = null;
    }
    const wavRecorder = wavRecorderRef.current;
    if(wavRecorder.getStatus() === "recording"){
      await wavRecorder.end();
    }
  };

  const handleVideoToggle = () => {
    setIsCameraOn((prev) => !prev);
    stopMediaStream();
    setTimeout(() => {
      startMediaStream();
    }, 500); // Restart the media stream with new constraints
  };

  const monitorAudioLevels = () => {
    const analyser = analyserRef.current;
    if (!analyser) return;

    const dataArray = new Uint8Array(analyser.frequencyBinCount);

    const checkAudio = () => {
      // analyser.getByteFrequencyData(dataArray);
      // const isVoice = dataArray.some((level) => level > 50); // Threshold for voice activity
      analyserRef.current?.getByteTimeDomainData(dataArray);

      // Calculate RMS value (volume level)
      const rms = Math.sqrt(
        dataArray.reduce((sum, value) => sum + Math.pow(value - 128, 2), 0) /
          dataArray.length
      );

      const threshold = 20; // Audio sensitivity threshold
      const isVoice = rms > threshold;
      // console.log("### VOICE: ", isVoice, "$$$ RECORDING: ", isRecording);
      if (isVoice) {
        if (!isRecording) {
          startRecording();
        }
        if (silenceTimeoutRef.current) {
          clearTimeout(silenceTimeoutRef.current);
          silenceTimeoutRef.current = null;
        }
      } else {
        silenceTimeoutRef.current = setTimeout(() => {
          const wavRecorder = wavRecorderRef.current;
          if(wavRecorder.getStatus() === "recording"){
            wavRecorder.end().then(() => {}).catch((error) => {})
          }
          stopRecording();
        }, 2000); // Stop after 2 seconds of silence
      }

      requestAnimationFrame(checkAudio);
    };

    checkAudio();
  };

  const startRecording = async() => {
      setIsRecording(true);
      console.log("^^^ RECORDING: ", videoChunksRef.current)
      const client = clientRef.current;
      const wavRecorder = wavRecorderRef.current;
      if(wavRecorder.getStatus() === "ended"){
        await wavRecorder.begin();
      }
      const wavStreamPlayer = wavStreamPlayerRef.current;
      const trackSampleOffset = await wavStreamPlayer.interrupt();
      if (trackSampleOffset?.trackId) {
        const { trackId, offset } = trackSampleOffset;
        await client.cancelResponse(trackId, offset);
      }
      
      if(wavRecorder.getStatus() === "recording"){
        await wavRecorder.record((data) => client.appendInputAudio(data.mono));
        // await wavRecorder.pause();
      }
      videoChunksRef.current = [];
  };

  const stopRecording = async() => {
    if (mediaRecorderRef.current && mediaRecorderRef.current?.state === "recording") {
      mediaRecorderRef.current?.stop();
      setIsRecording(false);
      const client = clientRef.current;
      const wavRecorder = wavRecorderRef.current;
      if(wavRecorder.getStatus() === "recording"){
        await wavRecorder.pause();
      }
      client.createResponse();
      console.log("&&& STOP RECORDING: ", true)
    }
  };

  const sendToOpenAI = async () => {
    console.log("User stopped speaking. Sending prompt...");
    const client = clientRef.current;
    if (client) {
      // Send a prompt message
    const title = "Road Safety"; // Replace with dynamic value
    const category = "Example Category"; // Replace with dynamic value
    const difficulty = "Medium"; // Replace with dynamic value
    const description = "A person is discussing road safety with an instructor about what things to carry within your vehicle at all times."; // Replace with dynamic value
    const mood = "Friendly"; // Replace with dynamic value
    const user_name = "User"; // Replace with dynamic value
    const previous_msg = "This is a sample scenario"

    const prompt = `
      Your task is to reply to the user based on previous chats, current user response, and the scenario with the following details:
      Title: ${title}, 
      Category: ${category}, 
      Difficulty: ${difficulty},
      Description: ${description},
      Mood: ${mood}.

      previous messages:
      ${previous_msg} // A function to fetch prior messages

      current message:
      Hello!

      If the last message is out of scenario context and not part of the scenario, create a dialog telling the user to get back to the current scenario. Do not respond to out-of-context messages.
      Name of the user is ${user_name}.

      Keep the conversation natural like a real person is talking.
      Return a single dialog.

      dialog
    `;
    client.sendUserMessageContent([
      {
        type: `input_text`,
        text: prompt,
      },
    ]);
    }
  };

  useEffect(() => {
    startMediaStream();
  }, [isCameraOn])

  useEffect(() => {
    // Get refs
    const wavStreamPlayer = wavStreamPlayerRef.current;
    const client = clientRef.current;

    // Set transcription, otherwise we don't get user transcriptions back
    client.updateSession({ input_audio_transcription: { model: 'whisper-1' } });

    client.on('conversation.updated', async ({ item, delta }: any) => {
      const items = client.conversation.getItems();
      if (delta?.audio) {
        // console.log('### audiodelta: ', delta.audio);
        wavStreamPlayer.add16BitPCM(delta.audio, item.id);
      }
      if (item.status === 'completed' && item.formatted.audio?.length) {
        const wavFile = await WavRecorder.decode(
          item.formatted.audio,
          24000,
          24000
        );
        item.formatted.file = wavFile;
      }
      setItems(items);
    });

    setItems(client.conversation.getItems());

    return () => {
      // cleanup; resets to defaults
      client.reset();
    };
  }, []); // ok till here

  /**
   * Connect to conversation:
   * WavStreamPlayer output, client is API client
   */
  const connectConversation = useCallback(async () => {
    const client = clientRef.current;
    const wavRecorder = wavRecorderRef.current;
    const wavStreamPlayer = wavStreamPlayerRef.current;

    // Set state variables
    startTimeRef.current = new Date().toISOString();
    setIsConnected(true);
    
    setItems(client.conversation.getItems());

    try {
      // Connect to audio output
      if(wavRecorder.getStatus() === "recording"){

      }else if(wavRecorder.getStatus() === "ended"){
        await wavRecorder.begin()
        sendToOpenAI();
      }else{
        await wavRecorder.record((data) => client.appendInputAudio(data.mono));
      }
      await wavStreamPlayer.connect();
    } catch (error) {
      console.error('connectConversation error:', error);
      // Ensure proper cleanup on error
      setIsConnected(false);
    }
  }, []);

  // Call the function when the component loads
  useEffect(() => {
    connectConversation();
    return () => {
      // Clean up resources on unmount
      if (audioContextRef.current?.state === "running") {
        audioContextRef.current.close();
      }
      if (streamRef.current) {
        const tracks = streamRef.current.getTracks();
        tracks.forEach((track) => track.stop());
      }
    };
  }, []);

  const convertToBase64 = (blob: Blob) => {
    const reader = new FileReader();
    reader.onloadend = () => {
      const base64String = reader.result as string;
      // setBase64Media(base64String);
    };
    reader.readAsDataURL(blob); // Convert to base64
  };

  const toggleMicrophone = () => {
    if (streamRef.current) {
      const audioTrack = streamRef.current.getAudioTracks()[0];
      if (audioTrack) {
        audioTrack.enabled = !audioTrack.enabled;
        setIsMicOn(audioTrack.enabled);
      }
    }
  };

  // Toggle the camera on/off
  // const toggleCamera = () => {
  //   console.log('Toggling camera...');

  //   if (!streamRef.current) {
  //     console.error('streamRef.current is null or undefined.');
  //     return;
  //   }

  //   const videoTracks = streamRef.current.getVideoTracks();
  //   if (videoTracks.length === 0) {
  //     console.error('No video tracks found in the stream.');
  //     return;
  //   }

  //   const videoTrack = videoTracks[0];
  //   videoTrack.enabled = !videoTrack.enabled;
  //   console.log(`Camera toggled: ${videoTrack.enabled}`);

  //   setIsCameraOn(!isCameraOn);
  // };
  // const toggleCamera = () => {
  //   if (streamRef.current) {
  //     const videoTrack = streamRef.current.getVideoTracks()[0];
  //     if (videoTrack) {
  //       videoTrack.enabled = !videoTrack.enabled;
  //       setIsCameraOn(videoTrack.enabled);
  //     }
  //   }
  // };

  // ---------------------

  const fetchChatApiSecondTime = async () => {
    const requestOptions: RequestInit = {
      method: 'GET',
      redirect: 'follow',
    };

    try {
      // const response = await fetch(
      //   'https://socialiq.zapto.org/show_chat?email=developer.wellorgs@gmail.com&scenario_id=67287e26933445b37471fe76&user_name=testuser&bot_name=Kevin',
      //   requestOptions
      // );
      const response = await fetch(
        'https://socialiq.zapto.org/show_chat?email=developer.wellorgs@gmail.com&scenario_id=67287c99933445b37471fe71&user_name=testuser&bot_name=Kevin&delete=true',
        requestOptions
      );
      const result = await response.json();
      if (result.bot_response) {
        setBotChat(result);
      } else if (result.time && result.chats.length > 0) {
        setChats((prevChats) => [...prevChats, ...result.chats]);
        setTime(result.time);
      }
    } catch (error) {
      console.error('Error fetching chats:', error);
    }
  };

  useEffect(() => {
    fetchChatApiSecondTime();
  }, []);

  const DeleteChatStatus = () => {
    const formdata = new FormData();
    formdata.append('email', 'developer.wellorgs@gmail.com');
    formdata.append('scenario_id', '67287c99933445b37471fe71');

    const requestOptions: RequestInit = {
      method: 'POST',
      body: formdata,
      redirect: 'follow',
    };

    fetch('https://socialiq.zapto.org/delete_chat_status', requestOptions)
      .then((response) => response.json())
      .then((result) => {
        if (result) {
          if (result.message == 'Added Delete status to the chat') {
            setIsOpen(false);
          }
        }
      })
      .catch((error) => console.error(error));
  };

  const addTimeChats = () => {
    const { hours, minutes, seconds } = timeData;
    const formattedTime = `${hours.toString().padStart(2, '0')}:${minutes
      .toString()
      .padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
    const formdata = new FormData();
    formdata.append('email', 'developer.wellorgs@gmail.com');
    formdata.append('scenario_id', '67287c99933445b37471fe71');
    formdata.append('time', formattedTime);

    const requestOptions: RequestInit = {
      method: 'POST',
      body: formdata,
      redirect: 'follow',
    };

    fetch('https://socialiq.zapto.org/add_time', requestOptions)
      .then((response) => response.json())
      .then((result) => {
        toast.success(result.message);
        setTimeUpdated(true);
        if (result) {
          const formdata = new FormData();
          formdata.append('email', 'developer.wellorgs@gmail.com');
          formdata.append('scenario_id', '67287c99933445b37471fe71');
          formdata.append('Title', 'Code Review Clash');
          formdata.append('Category', 'Conflict Resolution');
          formdata.append('Difficulty', 'Intermediate');
          formdata.append(
            'Description',
            'A tense conversation between a junior developer, User, and a senior developer, Jamie, over feedback on a code review.'
          );
          formdata.append('Mood', 'Supportive');
          formdata.append('user_name', 'hello');
          formdata.append('bot_name', 'hello');

          const requestOptions: RequestInit = {
            method: 'POST',
            body: formdata,
            redirect: 'follow',
          };

          fetch('https://socialiq.zapto.org/scenario_analysis', requestOptions)
            .then((response) => response.json())
            .then((result) => {
              setTimeout(() => {
                toast.success(result.Message);
              }, 5000);
            })
            .catch((error) => console.error(error));
        }
      })

      .catch((error) => console.error(error));
  };

  const fetchGetTips = async () => {
    const requestOptions: RequestInit = {
      method: 'GET',
      redirect: 'follow',
    };

    try {
      const response = await fetch(
        `https://socialiq.zapto.org/get_tips?email=developer.wellorgs@gmail.com&scenario_id=67287c99933445b37471fe71&Title=Code Review Clash&Category=Conflict Resolution&Difficulty=Intermediate&Description=A tense conversation between a junior developer, User, and a senior developer, Jamie, over feedback on a code review.&Mood= Supportive&user_name= Jamie&last_message=${text}`,
        requestOptions
      );
      const result = await response.json();
      if (result) {
        setgetTips(result);
      }
    } catch (error) {
      console.error('Error fetching chats:', error);
    }
  };

  const fetchStoreDetails = async () => {
    const myHeaders = new Headers();
    myHeaders.append('Content-Type', 'application/json');

    const raw = JSON.stringify({
      email: 'developer.wellorgs@gmail.com',
      Title: 'Code Review Clash',
      Category: 'Conflict Resolution',
      Difficulty: 'Intermediate',
      Description:
        'A tense conversation between a junior developer, User, and a senior developer, Jamie, over feedback on a code review.',
      Mood: 'Supportive',
      scenario_id: '67287c99933445b37471fe71',
      bot_name: 'hello',
      user_name: 'abc',
      start_message: 'Hello testuser',
      last_message: 'Letâ€™s make this',
    });

    const requestOptions: RequestInit = {
      method: 'POST',
      headers: myHeaders,
      body: raw,
      redirect: 'follow',
    };

    fetch('https://socialiq.zapto.org/store_details', requestOptions)
      .then((response) => response.json())
      .then((result) => {
        console.log(result.message);
        toast.success(result.message);
      })
      .catch((error) => console.error(error));
  };

  const fetchGenerateDialogVideo = () => {
    const formdata = new FormData();
    formdata.append('email', 'developer.wellorgs@gmail.com');
    formdata.append('scenario_id', '67287c99933445b37471fe71');
    formdata.append('Title', 'Code Review Clash');
    formdata.append('Category', 'Conflict Resolution');
    formdata.append('Difficulty', 'Intermediate');
    formdata.append(
      'Description',
      'A tense conversation between a junior developer, User, and a senior developer, Jamie, over feedback on a code review.'
    );
    formdata.append('Mood', 'Supportive');
    // formdata.append('video', wavBlobUrl);
    formdata.append('video', videoBlob!);
        // formdata.append('video', isCameraOn ? mediaBlob : wavBlobUrl );
    // formdata.append('is_video', wavBlobUrl ? 'true' : 'false');
    formdata.append('is_video', isCameraOn ? 'true' : 'false');

    const requestOptions: RequestInit = {
      method: 'POST',
      body: formdata,
      redirect: 'follow',
    };

    fetch('https://socialiq.zapto.org/generate_dialog_video', requestOptions)
      .then((response) => response.json())
      .then((result) => {
        console.log(result);
        setText('');
        setVideoUrl("");
        setVideoBlob(null)
      })
      .catch((error) => console.error(error));
  };
  useEffect(() => {
      if (videoBlob) {
        // // get TIP   GET API
        // fetchGetTips();
        // // send audio or video to gpt post api
        // fetchGenerateDialogVideo();
        // // store_details POST API
        // fetchStoreDetails();
      }
  }, [videoBlob]);
 
 
  return (
    <>
      <section className="bg-[#e6e6e6]  mx-auto py-3 px-6   ">
        <div className="">
          <div className="flex  items-start lg:justify-between flex-col lg:flex-row  lg:items-center">
            <p className="text-center text-[#1c4f78] text-sm ">
              <a href="/" title="home">
                Dashboard &gt;{' '}
              </a>
              <a href="/aboutus" title="about">
                Salary Negotation Success &gt;{' '}
              </a>
              <a href="/aboutus" title="about">
                Real Time Interaction{' '}
              </a>
            </p>

            <div className="flex   lg:justify-between lg:items-center lg:justify-center gap-5  md:flex-row py-3">
              <div className="flex items-center gap-5">
                {timeUpdated ? (
                  <TimerComponent
                    onTimeUpdate={setTimeData}
                    botAndChat={time || botChat?.time}
                  />
                ) : (
                  <TimerComponent onTimeUpdate={setTimeData} />
                )}
              </div>
              <Link
                className="inline-block w-auto text-center  px-6 py-1 text-white transition-all rounded-md shadow-xl sm:w-auto bg-[#5c9bb6] hover:shadow-2xl hover:shadow-blue-400 hover:-tranneutral-y-px "
                to="#"
                onClick={addTimeChats}
              >
                Exit
              </Link>
              <Link
                className="inline-block w-auto text-center px-5 py-1 text-white transition-all bg-gray-700 bg-[#ff5252] dark:text-white rounded-md shadow-xl sm:w-auto  hover:text-white shadow-neutral-300  hover:shadow-2xl hover:shadow-neutral-400 hover:-tranneutral-y-px"
                to="#"
                onClick={() => setIsOpen(true)}
              >
                End The Session
              </Link>
            </div>
            <DeleteModel
              open={isOpen}
              setOpen={setIsOpen}
              DeleteChatStatus={DeleteChatStatus}
            />
          </div>
        </div>
        {/* <div className="grid grid-cols-1 sm:grid-cols-12 gap-5"> */}
        <div className="grid sm:grid-cols-1 md:grid-cols-1 lg:grid-cols-12 gap-4">
          <div className="sm:col-span-6  lg:col-span-7 xl:col-span-7  ">
            <div className="flex gap-5">
              <img
                src={imagess}
                className="rounded-2xl w-1/2 h-80 object-cover "
                style={{ borderWidth: 0, borderColor: 'blue' }}
              />
              <div>
                {/* Webcam feed display */}
                <div style={{ position: 'relative' }} className="w-full h-full">
                  {/* <h2>Webcam Feed</h2> */}
                  <video
                    ref={videoRef}
                    autoPlay
                    muted
                    width="300"
                    height="400"
                    className="border w-full h-full shadow-md object-cover"
                    style={{ borderRadius: '16px' }}
                  ></video>

                  {/* Mic and Camera control buttons */}
                  <div className="flex justify-center items-center gap-5 absolute bottom-0 inset-x-0">
                    <div>
                      <button
                        onClick={toggleMicrophone}
                        style={{ background: 'transparent', border: 'none' }}
                      >
                        {isMicOn ? (
                          <FaMicrophone size={20} color="white" />
                        ) : (
                          <FaMicrophoneSlash size={20} color="white" />
                        )}
                      </button>
                    </div>
                    <div>
                      <button
                        onClick={handleVideoToggle}
                        style={{ background: 'transparent', border: 'none' }}
                      >
                        {isCameraOn ? (
                          <FaVideo size={20} color="white" />
                        ) : (
                          <FaVideoSlash size={20} color="white" />
                        )}
                      </button>
                    </div>
                  </div>
                </div>
              </div>
            </div>

            <div className="flex lg:justify-end  mt-5">
              <div className=" bg-[#e3f2fd] lg:w-[360px]  rounded-2xl p-5">
                <span>{getTips !== null && getTips.Tip}</span>
                <span className="block">
                  {getTips !== null && getTips.emoji}
                </span>
              </div>
            </div>
            {text && (
              <div>
                <h2>Base64 Audio & Video</h2>
              </div>
            )}
          </div>

          <div className="max-w-2xl flex justify-center sm:col-span-6 lg:col-span-5 xl:col-span-5">
            <div className="bg-white  h-[86vh] shadow-lg overflow-y-scroll   pb-10 border border-1 border-zinc-300 border-opacity-30 rounded-2xl relative overflow-hidden p-5">
              <div className="flex flex-col w-full gap-3">
                {botChat ? (
                  <>
                    <div className="flex justify-start gap-1">
                      <img
                        src={
                          botChat.avatarUrl ||
                          'https://www.tailwind-kit.com/images/object/10.png'
                        }
                        className="h-10 w-10 border border-[1px] border-zinc-300 border-opacity-50 rounded-full ml-3 opacity-90 "
                      />
                      <div className="w-1/2 bg-[#2196f3] border border-1 border-zinc-300 border-opacity-30 rounded-lg flex items-center px-2 py-2 text-white relative">
                        {botChat.bot_response}
                      </div>
                    </div>

                    <div className="mt-2 ">
                      {/* <audio autoPlay>
                        <source src={botChat?.audio_url} type="audio/mp3" />
                        Your browser does not support the audio element.
                      </audio> */}
                    </div>
                  </>
                ) : chats.length > 0 ? (
                  chats.map((message, index) => {
                    const isTestUserMessage = message.startsWith('testuser:');
                    return (
                      <div
                        key={index}
                        className={`flex ${
                          isTestUserMessage ? 'justify-end' : 'justify-start'
                        } items-center`}
                      >
                        {isTestUserMessage ? (
                          <>
                            <div className="w-1/2 bg-[#2196f3] border border-1 border-zinc-300 border-opacity-30 rounded-lg flex items-center px-2 py-2 text-white relative text-sm">
                              {message}
                            </div>
                          </>
                        ) : (
                          <>
                            <img
                              src={
                                message.avatarUrl ||
                                'https://www.tailwind-kit.com/images/object/10.png'
                              }
                              className="h-10 w-10 border border-[1px] border-zinc-300 border-opacity-50 rounded-full mr-3 opacity-90"
                              alt="User avatar"
                            />
                            <div className="w-1/2 bg-[#eee] border border-1 border-zinc-300 border-opacity-30 rounded-md flex items-start px-2 py-2 text-black relative text-sm">
                              {message}
                            </div>
                          </>
                        )}
                      </div>
                    );
                  })
                ) : (
                  <div>Loading chats.............</div> // In case there's no response or chat
                )}
              </div>
              {items.map((conversationItem, i) => {
                return (
                  <div className="" key={conversationItem.id}>
                    <div className={``}>
                      {/* tool response */}
                      {/* {conversationItem.type === 'function_call_output' && (
                        <div>{conversationItem.formatted.output}</div>
                      )} */}
                      {/* tool call */}
                      {/* {!!conversationItem.formatted.tool && (
                        <div>
                          {conversationItem.formatted.tool.name}(
                          {conversationItem.formatted.tool.arguments})
                        </div>
                      )} */}

                      {!conversationItem.formatted.tool &&
                          conversationItem.role === 'user' && (
                            <div>
                              {conversationItem.formatted.transcript ?? "--"}
                            </div>
                         )}

                      <div className="flex mb-4 justify-end gap-1 ">
                        <div className="w-1/2 bg-[#eee] border border-1 border-zinc-300 border-opacity-30 rounded-md flex items-start px-2 py-2 text-black relative">
                          {/* {conversationItem.formatted.transcript ||
                            conversationItem.formatted.text ||
                            '(truncated)'} */}

                          {/* {text} */}

                          {/* {!conversationItem.formatted.tool &&
                            conversationItem.role === 'user' && (
                              <div>
                                {
                                  conversationItem.formatted.text}
                              </div>
                            )} */}
                        </div>
                      </div>

                      {!conversationItem.formatted.tool &&
                        conversationItem.role === 'assistant' && (
                          <div className="flex mb-4 justify-start gap-1">
                            <img
                              src={
                                'https://www.tailwind-kit.com/images/object/10.png'
                              }
                              className="h-12 w-12 border border-[1px] border-zinc-300 border-opacity-50 rounded-full ml-3 opacity-90 pl-[1px]"
                            />
                            <div className="w-1/2 bg-[#2196f3] border border-1 border-zinc-300 border-opacity-30 rounded-lg flex items-center px-2 py-2 text-white relative">
                              {conversationItem.formatted.transcript ||
                                conversationItem.formatted.text ||
                                '(truncated)'}
                            </div>
                          </div>
                        )}
                    </div>
                  </div>
                );
              })}
            </div>
          </div>
        </div>
      </section>
    </>
  );
}

export default ScenarioForm;
